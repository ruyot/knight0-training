"""
Download and process the Kaggle chess dataset (109M positions).

This dataset is PERFECT for training:
- 109M labeled positions
- Already has eval + result
- High quality engine data
"""

import kagglehub
import pandas as pd
import pickle
import chess
from pathlib import Path
from tqdm import tqdm
import numpy as np

print("="*80)
print("ðŸ“¥ DOWNLOADING KAGGLE CHESS DATASET (109M POSITIONS)")
print("="*80)
print("")

# Download dataset
print("Downloading from Kaggle (this may take a while - 9.79 GB)...")
path = kagglehub.dataset_download("joannpeeler/labeled-chess-positions-109m-csv-format")
print(f"Dataset downloaded to: {path}")
print("")

# Find the CSV file
dataset_path = Path(path)
csv_files = list(dataset_path.glob("*.csv"))

if not csv_files:
    print("Error: No CSV files found!")
    exit(1)

csv_file = csv_files[0]
print(f"Found dataset: {csv_file}")
print(f"   Size: {csv_file.stat().st_size / (1024**3):.2f} GB")
print("")

# Process in chunks to save memory
print("Processing dataset (converting to training format)...")
print("   This will take 10-20 minutes...")
print("")

chunk_size = 100000  # Process 100K rows at a time
output_chunks = []
total_positions = 0
max_positions = None 

print(f"Target: ALL POSITIONS (~109M)")
print("")

for chunk_num, chunk in enumerate(pd.read_csv(csv_file, chunksize=chunk_size)):
    if max_positions and total_positions >= max_positions:
        break
    
    # Extract relevant columns
    # CSV has: Hash, Ply, GamePly, FEN, HasCastled, Eval, Result
    positions = []
    
    for idx, row in chunk.iterrows():
        if total_positions >= max_positions:
            break
        
        try:
            fen = row['FEN']
            eval_score = row['Eval']  # Centipawn score
            result = row['Result']  # 0 = loss, 0.5 = draw, 1 = win
            
            # Create board to get legal moves
            board = chess.Board(fen)
            
            # Get best move (we'll use stockfish's eval to guide policy)
            # For now, we'll use the eval to create a value target
            # and sample moves uniformly (model will learn from eval)
            
            legal_moves = list(board.legal_moves)
            if not legal_moves:
                continue
            
            # Pick a legal move (model will learn which are best from training)
            move = legal_moves[0]  # Just use first legal move for now
            
            # Normalize value
            # Result is already 0/0.5/1, which is perfect for value head!
            value = result
            
            positions.append({
                "board": fen,
                "move": move.uci(),
                "value": float(value * 2 - 1)  # Convert 0/0.5/1 -> -1/0/1
            })
            
            total_positions += 1
            
        except Exception as e:
            continue
    
    if positions:
        output_chunks.append(positions)
    
    # Progress update
    if (chunk_num + 1) % 10 == 0:
        print(f"  Processed {total_positions:,} positions...")

print("")
print(f"Processed {total_positions:,} positions")
print("")

# Combine all chunks
print("Saving to pickle format...")
all_positions = []
for chunk in output_chunks:
    all_positions.extend(chunk)

output_file = Path("kaggle_training_data.pkl")
with open(output_file, 'wb') as f:
    pickle.dump(all_positions, f)

print(f"Saved to: {output_file}")
print(f"  Size: {output_file.stat().st_size / (1024**2):.1f} MB")
print(f"  Positions: {len(all_positions):,}")
print("")

print("="*80)
print("KAGGLE DATASET READY FOR TRAINING")
print("="*80)
print("")
print("Next steps:")
print("  1. Upload to Modal: modal volume put knight0-volume kaggle_training_data.pkl /kaggle_training_data.pkl")
print("  2. Run training: modal run train_on_kaggle.py")
print("")

